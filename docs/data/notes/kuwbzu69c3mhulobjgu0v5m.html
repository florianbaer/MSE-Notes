<h1 id="textclassification"><a aria-hidden="true" class="anchor-heading" href="#textclassification"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Textclassification</h1>
<h1 id="text-classification"><a aria-hidden="true" class="anchor-heading" href="#text-classification"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Text Classification</h1>
<p>Sentitment analysis is not simple text classification</p>
<ul>
<li>
<p>Lot of people still do hand coded rules to classify text: Needs a lot of work (expert knowledge)</p>
</li>
<li>
<p>Supervised machine learning (Labeling is required)</p>
<ul>
<li>Learn the relationship between text and label</li>
</ul>
</li>
<li>
<p>a set of document <span class="math math-inline">D</span></p>
</li>
<li>
<p>a fixed set of <span class="math math-inline">N</span> classes <span class="math math-inline">C</span></p>
</li>
<li>
<p>a training set of <span class="math math-inline">M</span> hand-labeled documents (<span class="math math-inline">d_1</span>, <span class="math math-inline">c_1</span>), . . . ,(<span class="math math-inline">d_M, c_M</span>)</p>
</li>
<li>
<p>a mapping <span class="math math-inline">D \to C</span> that associates a predicted class <span class="math math-inline">c \in C</span> to each
document <span class="math math-inline">d \in D</span></p>
</li>
<li>
<p>Naïve Bayes</p>
</li>
<li>
<p>Logistic regression</p>
</li>
<li>
<p>Support-vector machines (SVM)</p>
</li>
<li>
<p>k-Nearest Neighbors</p>
</li>
</ul>
<p><img src="/assets/images/2022-03-03-09-28-57.png" alt="Naïve Bayes concept">
Its called naive because it assumes that the features are independent of each other.
<img src="/assets/images/2022-03-03-09-31-50.png" alt="Bayes theorem"></p>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">MAPClassifier (Private)</a> is also called maximum a posteriori (MAP) classifier.</p>
<p><span class="math math-inline">argmax_{c \in C} \frac{P(d|c)P(c)}{P(d)} = argmax_{c \in C} P(d|c)P(c)</span></p>
<p>Bernoulli model (if word is in document) or multinominal model when <span class="math math-inline">f_i</span> is a count of the number of times the word <span class="math math-inline">i</span> appears in the document.</p>
<p>Naive bayes is the probability of class c with the product of conditioanl probabilities of each word in the document.</p>
<p><span class="math math-inline">c_{nb} = \text{arg max}_{c \in C} P(c) \product_{k = 1}^N P(x_k | c)</span></p>
<p><strong>Slide 18</strong> every class observed <span class="math math-inline">\frac{1}{3}</span></p>