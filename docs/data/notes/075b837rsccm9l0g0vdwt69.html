<h1 id="simplelinearregression"><a aria-hidden="true" class="anchor-heading" href="#simplelinearregression"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Simplelinearregression</h1>
<h1 id="simple-linear-regression"><a aria-hidden="true" class="anchor-heading" href="#simple-linear-regression"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Simple linear regression</h1>
<p>we get the 95% <a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">confidence interval (Private)</a> with the following formula: [<span class="math math-inline">\hat{\beta_1} - 2 * \text{se}(\hat{\beta_1}), \hat{\beta_1} + 2 * \text{se}(\hat{\beta_1})</span>]</p>
<p><span class="math math-inline">\epsilon = Y - (\beta_0 + \beta_1 * X)</span> cannot be measured since <span class="math math-inline">\beta_0</span> and <span class="math math-inline">\beta_1</span> are unknown.</p>
<p>Approximation for residuals for <span class="math math-inline">\epsilon</span> <strong>residuals</strong> <span class="math math-inline">r_i = y_i - (\beta_0 + \beta_1 * x_i)</span></p>
<p><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">RSE (Private)</a> Residual Standard Error = <span class="math math-inline">\text{RSE} = \sqrt{\frac{r_1^2 + ... + r_n^2}{n - 2}}</span></p>
<p>Prediction Interval = <span class="math math-inline">\text{se}(y_0)^2 = \sigma^2 (1 + \frac{1}{n}+ \frac{(x_0 - \bar{x}^2)}{\Epsilon_{i = 1}^2 (x_i - \bar{x})^2})</span></p>
<h1 id="model-assumptions-for-the-error-term"><a aria-hidden="true" class="anchor-heading" href="#model-assumptions-for-the-error-term"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Model Assumptions for the Error Term</h1>
<p><span class="math math-inline">\epsilon_i\text{ iid }\N(0,\sigma^2)</span></p>
<h2 id="aim-of-residual-analysis"><a aria-hidden="true" class="anchor-heading" href="#aim-of-residual-analysis"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Aim of residual analysis</h2>
<p>If the model contains errors - we get a starting point for further investigation to a better and more adapted model (explorative data analysis)</p>
<p><strong>RSE</strong> considered a measure of the <strong>lack of fit</strong> of a regression model to the data.
<strong><span class="math math-inline">R^2</span></strong> Statistic is defined as : <span class="math math-inline">R^2 = 1 - \frac{\Epsilon_{i = 1}^n (y_i - \hat{y_i})^2}{\Epsilon_{i = 1}^n (y_i - \bar{y_i})^2} = 1 - \frac{\text{variance left after regression fit}}{\text{total variance}}</span></p>
<p><span class="math math-inline">R^2</span> is always between 0 and 1 (0 is bad, 1 is a perfect model)</p>
<h1 id="correlation-coefficient-private"><a aria-hidden="true" class="anchor-heading" href="#correlation-coefficient-private"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a><a title="Private" style="color: brown" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank">Correlation Coefficient (Private)</a></h1>
<p><span class="math math-inline">r = Cor(X,Y) = \frac{\Epsilon_{i = 1}^n (x_i - \bar{x_i})(y_i - \bar{y_i})}{\sqrt(\Epsilon_{i = 1}^n (x_i - \hat{x_i})^2)\Epsilon_{i = 1}^n (y_i - \bar{y_i})^2)}</span></p>
<p>You can not use the Correlation because of Multiple Linear Regression</p>
<h1 id="diagnostics-tool-for-testing-model-assumption---tukey-anscombe-plot"><a aria-hidden="true" class="anchor-heading" href="#diagnostics-tool-for-testing-model-assumption---tukey-anscombe-plot"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Diagnostics Tool for Testing Model Assumption - Tukey-Anscombe-Plot</h1>
<p>Verify that <span class="math math-inline">E[\epsilon] = 0</span></p>
<h2 id="tukey-anscombe-plot"><a aria-hidden="true" class="anchor-heading" href="#tukey-anscombe-plot"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Tukey-Anscombe-Plot</h2>
<p>Do it by the use of the <strong>Tukey-Anscombe-Plot</strong></p>
<ul>
<li>Residuals ploted on the vertical axis</li>
<li>predicted values on the horizontal axis</li>
</ul>
<p>Values should be in a horizontal line at Residuals = 0</p>
<p>Simulate Points to get more points </p>
<p><img src="assets/images/tukey_ascom.png"></p>
<h2 id="scale-location-plot"><a aria-hidden="true" class="anchor-heading" href="#scale-location-plot"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Scale-Location Plot</h2>
<p>Use the Standardized residuals </p>
<h3 id="consequences-for-case-of-correlated-error-terms"><a aria-hidden="true" class="anchor-heading" href="#consequences-for-case-of-correlated-error-terms"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Consequences for case of correlated error terms</h3>
<ul>
<li>The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of independent error terms <span class="math math-inline">\epsilon_i</span></li>
<li>If there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be</li>
</ul>
<h1 id="outlier"><a aria-hidden="true" class="anchor-heading" href="#outlier"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Outlier</h1>
<p>An outlier is if a data point is very far away from the most of the data.
If we remove an outlier - we have a small effect on the <span class="math math-inline">\beta_0</span> and <span class="math math-inline">\beta_1</span> but the effects on the <strong>RSE</strong> and <span class="math math-inline">R^2</span> is large</p>
<h2 id="high-leverage-points"><a aria-hidden="true" class="anchor-heading" href="#high-leverage-points"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>High Leverage points</h2>
<p><img src="assets/images/high_leverage_points.png">
If we remove observation 41 - the red line is the correct point - so if we remove high leverage points, the impact on <span class="math math-inline">\beta_0</span> and <span class="math math-inline">\beta_1</span> is big.</p>
<h2 id="cooks-distance"><a aria-hidden="true" class="anchor-heading" href="#cooks-distance"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Cooks distance</h2>
<p><span class="math math-inline">\hat{y}_{-i}</span> obeservation if you remove the i-th value.
As higher the value of cooks distance is - the higher is the influence of the i-th data point on the dataset
If a cooks value higher than 1 is observed - the value is considered as dangerously influencual.</p>
<p><strong>Read more about Tukey first aid principles</strong></p>
<p><strong>If you discover outliers and high leverage points - do not just remove them - it could be an important discovery for a new and other model!!!</strong></p>
<ol>
<li>Check wether outlier has occured due to an error in data collecito or recording
<ol>
<li>If an error may have occured: omit the data point</li>
<li>if an error can be excluded: go to 2</li>
</ol>
</li>
<li>Attempt to transform predictor or response variables in order to make <em>disappear</em> the outlier. If no improvement go to 3.</li>
<li>Outlier occured due to an unusual random varianions: If such outlier affect parameter estimation too much, then the observation my be removed (<strong>needs to be mentioned in the reports</strong>)</li>
</ol>