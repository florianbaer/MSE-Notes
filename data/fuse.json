{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["body"],"id":"body","weight":1,"src":"body"}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Root","n":1},"1":{"v":"# Welcome to Dendron\n\nThis is the root of your dendron vault. If you decide to publish your entire vault, this will be your landing page. You are free to customize any part of this page except the frontmatter on top.\n\n## Lookup\n\nThis section contains useful links to related resources.\n\n- [Getting Started Guide](https://link.dendron.so/6b25)\n- [Discord](https://link.dendron.so/6b23)\n- [Home Page](https://wiki.dendron.so/)\n- [Github](https://link.dendron.so/6b24)\n- [Developer Docs](https://docs.dendron.so/)\n","n":0.132}}},{"i":2,"$":{"0":{"v":"Templates","n":1}}},{"i":3,"$":{"0":{"v":"Mse","n":1}}},{"i":4,"$":{"0":{"v":"asdf","n":1},"1":{"v":"[[MSE]] Topic: \n","n":0.577}}},{"i":5,"$":{"0":{"v":"MSE","n":1},"1":{"v":"\n# Master of Science in Engineering","n":0.408}}},{"i":6,"$":{"0":{"v":"Ss22","n":1}}},{"i":7,"$":{"0":{"v":"Delearn","n":1}}},{"i":8,"$":{"0":{"v":"Practical Work","n":0.707},"1":{"v":"\n\n# Semester week 1\n**Responsible for week Flo**\n- Exercise 1 is done by Mätthu\n- Exercise 2 is done by Manuel\n- Exercise 3 is done by Adrian\n- Exercise 4 is done by Flo\n# Semester week 2\n**Responsible for week Mätthu**\n- Exercise 1 is done by Manuel\n- Exercise 2 is done by Adrian\n- Exercise 3 is done by Flo\n- Exercise 4 is done by Mätthu\n# Semester week 3\n**Responsible for week Manuel**\n- Exercise 1 is done by Adrian\n- Exercise 2 is done by Flo\n- Exercise 3 is done by Mätthu\n- Exercise 4 is done by Manuel\n# Semester week 4\n**Responsible for week Adrian**\n- Exercise 1 is done by Flo\n- Exercise 2 is done by Mätthu\n- Exercise 3 is done by Manuel\n- Exercise 4 is done by Adrian\n# Semester week 5\n**Responsible for week Flo**\n- Exercise 1 is done by Mätthu\n- Exercise 2 is done by Manuel\n- Exercise 3 is done by Adrian\n- Exercise 4 is done by Flo\n# Semester week 6\n**Responsible for week Mätthu**\n- Exercise 1 is done by Manuel\n- Exercise 2 is done by Adrian\n- Exercise 3 is done by Flo\n- Exercise 4 is done by Mätthu\n# Semester week 7\n**Responsible for week Manuel**\n- Exercise 1 is done by Adrian\n- Exercise 2 is done by Flo\n- Exercise 3 is done by Mätthu\n- Exercise 4 is done by Manuel\n# Semester week 8\n**Responsible for week Adrian**\n- Exercise 1 is done by Flo\n- Exercise 2 is done by Mätthu\n- Exercise 3 is done by Manuel\n- Exercise 4 is done by Adrian\n# Semester week 9\n**Responsible for week Flo**\n- Exercise 1 is done by Mätthu\n- Exercise 2 is done by Manuel\n- Exercise 3 is done by Adrian\n- Exercise 4 is done by Flo\n# Semester week 10\n**Responsible for week Mätthu**\n- Exercise 1 is done by Manuel\n- Exercise 2 is done by Adrian\n- Exercise 3 is done by Flo\n- Exercise 4 is done by Mätthu\n# Semester week 11\n**Responsible for week Manuel**\n- Exercise 1 is done by Adrian\n- Exercise 2 is done by Flo\n- Exercise 3 is done by Mätthu\n- Exercise 4 is done by Manuel\n# Semester week 12\n**Responsible for week Adrian**\n- Exercise 1 is done by Flo\n- Exercise 2 is done by Mätthu\n- Exercise 3 is done by Manuel\n- Exercise 4 is done by Adrian\n# Semester week 13\n**Responsible for week Flo**\n- Exercise 1 is done by Mätthu\n- Exercise 2 is done by Manuel\n- Exercise 3 is done by Adrian\n- Exercise 4 is done by Flo\n# Semester week 14\n**Responsible for week Mätthu**\n- Exercise 1 is done by Manuel\n- Exercise 2 is done by Adrian\n- Exercise 3 is done by Flo\n- Exercise 4 is done by Mätthu\n","n":0.049}}},{"i":9,"$":{"0":{"v":"Overfitting_and_regularization","n":1},"1":{"v":"\n[[mse.ss22.sw04]]\n[[MSE]] Topic: [[DeLearn]]\n\n## Recap\n\n## Learning Model from Data\n\nform probability distribution denoted by $ Z \\approx P(\\cdot)$\n\nsupervised learning\n- features with given labels\n\nunspuervised learning\n- e.g. density estimation without labels\n\ndefine a loss function $f,z \\to L(f,z) \\ge 0$\n- CE Loss\n- MSE Loss\n\nminimize statistical / expected risk of the model\n### Statistical Risk\nNot tractable (handhabbar)\n\n### Empirical Risk, Cost\n\ncreate a model from samples and sum the loss of all the sample \n![Empirical Risk (Cost)](/assets/images/2022-03-17-13-34-13.png)\nDoes not generalize \n\n### Parametric Empirical Risk minimazation\n\nTransform model into parametric model to find optimal parameters\n\n### Shaping solutions\n- increate numebr of samples\n- selection of a suitable class of parametric model\n  - crucial that the solution work with new data\n  - choose the most simple model if multiple models performe equally well (what is simple?)\n- adjust the loss function (make the solution less dependent on the training data)\n  - regularization\n  - margin maximization\n  - ensemble methods\n\n### Error Decomposition\nError contains the following components:\n- variance\n- bias\n![Variance and Bias in Model](/assets/images/2022-03-17-13-48-46.png)\nwhich both are dependent on the performance of the model\n\nVisual overfitting for further analysis as in the image below\n![Furhter analysis of overfitting](/assets/images/2022-03-17-14-03-39.png)\n\n**Neural Networks are rather low variance learners**\n![Neural Network learning](/assets/images/2022-03-17-14-12-01.png)\n\n\n### Deep double descent\nProblem of neural Networks which increase performance with more layers. But inference time is decreasing with more layers.\n![deep double descent](/assets/images/2022-03-17-14-36-32.png)\n\n# Overfitting problems \n\nRegularization methods\n\n- contrain model parameters (weight penalty)\n- early stopping (before high variance occurs)\n- add noise to training process\n- increase variety in training data\n## weight penalty\n$C = C_0 + \\lambda\\Omega(W) \\quad \\text{where} \\quad (\\lambda \\ge 0)$ \n\n## Gradient Descent\n\n![Gradient Descent with weight decay](/assets/images/2022-03-17-14-52-37.png)\n\nthere is $L_1$ and $L_2$ regularization. \ngreen is $L_1$ and red is $L_2$\n\n![Regularization](/assets/images/2022-03-17-15-00-25.png)\n\n## Dropout\nVery versatile (vielseitig)\n\nExample of dropout $\\to$ flip coin if you should go to work. Company would be less dependent on single employees but has to adapt the organization.\n\n# Model selection\n- Big model\n  - 98% / 1% / 1%\n- small model\n  - 60% / 20% / 20%\n\n## Hyper Parameters\n- number of neurons\n- number of layers\n- learning rate\n- batch size\n- epochs\n- dropout\n- weight decay (regularization)\n\n## Learning curve analysis\n\n## k-fold cross validation\n\n\n\n\n\n\n\n\n\n\n\n\n","n":0.055}}},{"i":10,"$":{"0":{"v":"Multi-Layer Perceptron","n":0.707},"1":{"v":"[[mse.ss22.sw03]]\n# Recap\nFirst forward propagation and then the backward propagation\n\nIn Mini-Batch GD - you get further performance by using a mini-batch of data.\nCrossEntropy does not use the softmax output but it uses the logits.\n\n**Keep a n eye on the shapes of the Tensors** - print shapes etc...\n\n# Multi Layer Perceptrons (MLP)\n\n## Rosenblatt (1958)\nOnly returns values {0, 1}\nNeeds to be linearly separable and works only with one perceptron\nXOR Problem not solved by Rosenblatt perceptron as it is not linearly seperable\n\n- sigmoid \n- tanh\n- Recti-Linear Unit (ReLU)\n\nMLP are not linear anymore as we have the non-linear activation functions\n\n## Forward pass\n\nWe have forward and backward pass \n- forward - calc results\n- backwards - calc gradients\n\nBackProp was formulated in the 1975\nCompuntational Graph possible\n\n## MLP Layer\n1. Activation from previous layer\n2. Weights and Bias caluclated\n3. Activation applied to weights and bias\n\nWeights are in a $n_l * n_{l-1}$ matrix\n\n![Composed Graphs](/assets/images/2022-03-10-13-48-27.png)\n\nImplies nested function calls\n\n## Chain rule\n![Chain rule -nested](/assets/images/2022-03-10-13-50-47.png)\nChain rule because we have nested function calls\n\n$$f'(g(x)) = f'(g(x))*g'(x)$$\n$$J_{f o g(x)}(x) = J_f(g(x)) * J_g(x)$$ jacobian\n\nSlide 33 is good for learning\n\nStarting with the green derivative and then go to the green one which gives teh result of all these derivatives (g\\*r\\*b)\n\n![MLP with two inputs](/assets/images/2022-03-10-13-58-35.png)\nWe have two input variables and therefore ned to multiply the deriviatives\n\n![MLP with two outputs](/assets/images/and%20now%20wit.png)\nhere the calculation is done with two outputs - apply them together as the sum\n\n\n\n","n":0.067}}},{"i":11,"$":{"0":{"v":"Learningandoptimization","n":1},"1":{"v":"# Learning and Optimization\n\nTarget for this week: Gradient Descent / stochastic gradient descent\n\n\n## General Regression Problem\n\nLearning a numeric value from a set of datapoints.\n\n## Classification Problem\n\nClassification of a set of datapoints into a set of classes.\n\nFull Batch Gradient Descent\nMini-Batch Gradient Descent or Stochastic Gradient Descent\n- You need to make the learning rate smaller with stochastic gradient descent\n- Less epochs needed - as per Batch the calculation is done\n- Calculation-Time is increaed per Epoch, as calculation is done per batch\n\n## Hyperparameters\n- Batch Size\n  - $\\epsilon \\sim \\frac{1}{\\sqrt{\\text{batchsize}}}$\n- Learning rate\n  - $\\epsilon \\sim \\alpha$\n\nError bars scale with the learning rate $\\alpha$\n\nWatch `torch.nn.CrossEntropyLoss`\n\n# ToDos\n- [ ] Watch the math behind DeLearn","n":0.096}}},{"i":12,"$":{"0":{"v":"Introduction DeLearn","n":0.707},"1":{"v":"# Introduction\n\n[[MSE]] [[DeLearn]]\n## What is ML ?\n1. Learning from data\n\n## Why do we need ML?\n1. We dont know the rules\n2. where math stops\n3. very complex problems\n\n## ML apps?\n1. Object detection\n2. Generate new data\n3. High dimensional data (images, sound)\n\n## DL $\\ne$ ML?\n1. DL is learning from feature extraction\n2. DL is a part of ML\n\n## Dangers of DL?\n1. Biased data\n2. AI explainability\n3. Don't rely to much on the models\n\n# Definitions\n> Machine learning consists computer methods that analyse observation data to automatically detect patterns, and then use the uncovered patterns to perform tasks based on new unobserved data.\n\n> “[Machine Learning is the] fi eld of study that gives computers the ability to learn without being explicitly programmed.” - Arthur Samuel, 1959\n\n> Machine Learning could be defi ned as a set of methods that automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty. - Machine Learning – A Probabilistic Perspective, K. Murphy, 2012.\n\n> A machine learning program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. - Machine Learning, T. Mitchell, 1997.\n\n![Workflow repetitions in MachineLearning](assets/images/DeLearn_1_RepeateLearning.png)\n\n## Supervised Machine Learning \n> With supervised learning, the goal is to extract some relevant features x from raw observation data o and to learn a mapping from inputs x to outputs y given a set of example data called the training set. \n\n![Introduction](assets/images/Introduction_HowProceed.png)\n\n**PROBLEM 1**: We need large quantities of human validated examples! …and this is costly to build\n\n**PROBLEM 2**: Because of the variabilities, we will need even more data and the mapping functions need to capture more complexities. \n\n**PROBLEM 3**: We usually spend a lot of time to hand-craft interesting compact features, this is called feature engineering\n\n","n":0.057}}},{"i":13,"$":{"0":{"v":"BackPropagation Algorithm","n":0.707},"1":{"v":"[[mse.ss22.sw03]]\n","n":1}}},{"i":14,"$":{"0":{"v":"Datamgmt","n":1}}},{"i":15,"$":{"0":{"v":"Sw01","n":1},"1":{"v":"# Postgres \nIs a post-relational extensinble DBMS - similar to oracle, ms sql server, ingers. Close to SQL standard but \"conservative\" \nwith special types for e.g. spatial data etc...\nAnd even further extensible with extensions (e.g. PostGIS)\n- **Good total costs of ownership**\n- Not so good mobile mobile sync\n- Not so good html form generator\n\n\n[[mse.ss22.datamgmt.cte]]\n\n[[mse.ss22.datamgmt.arraytypes]]\n","n":0.137}}},{"i":16,"$":{"0":{"v":"Key-Value Stores","n":0.707},"1":{"v":"# Key value stores\n[[mse.ss22.sw03]]\n\nOne way of usig it in postgres is `hstore`\n\n```sql\n--enable exteinos\ncreate extension hstore\n\n-- delete extension \ndrop extension hstore\n\n-- usage\nselect 'a=>x, b=>y'::hstore -> 'a' -- is an extension delivered with postgres-contrib\n```\n\nDictionary is an abstract data structure which contains a value or several values.\n\nKey values pairs are used to make a schema-less / schema-later / open schema model.\n\n- **Possibly suitable for**\n  - Easy data storage and data capture\n  - Rows with many attributes that are rarely examined\n  - Semi-structured data\n- **Tips**\n  - Try to avoid reporting against big heaps of Dictionaries\n  - Consider additional attributes with lookup keys (see Free Text Search)\n  - Consider running triggers or a post-process to transform KVPs into an entity-based schema.\n\n\n\n```sql\nselect 'a => 1, a=>2'::hstore; -- only saves the first occurance \n-- \"a\" => \"1\", so value 2 is ignored\n\nselect akeys('key3=>3, key2=>2'::hstore)\n-- [key3, key2]\n\nselect 1 as id, each('key3=>3, key2=>2'::hstore) -- returns tuples\n\nselect each(kykvpfield) from ... -- returns all kvp as tuples\n```\n\n# PostGIS\n\n```sql\nselect *, tags->'tourism' as featureclass\nfrom osm_location where tags->'tourism' in ('camp_site','caravan_site');\n-- find camping sites on https://terminal.osmdatapipeline.geoh.infs.ch/\n```\n\n# Tree\n\nA tree is a graph without cycles\n\n- Parent / Child / Ancestor Nodes\n- Sibling Nodes\n- Root Node: Only Node which does’nt have a parent\n- Leaf Node: Node which has no child\n- Internal Node: Node which is neither the root nor the leaf node\n- Level of a Node (depth)\n- Degree of a Node: No. of children a node has\n- Height of Tree: Max. level of nodes in that tree\n- Subtree of a Tree\n\n**Postgres even has Ltree**\n\n![Tree Data Structures](/assets/images/2022-03-09-14-44-39.png)\n\n# JSON\nPostgreSQL knows JSON and JSONB\n- JSON: Textual storage «as is», multiple keys\n- JSONB: Internal binary, one key, indexable\n\n**use jsonb_pretty() for printing**\n\nAccessors: ->, ->> #>, #>>\nProcessing: @>\n\n```sql\nselect '{\"a\":1, \"b\":2, \"c\":3}'::jsonb @> '{\"c\":3, \"b\":2}'::jsonb\n-- returns ture\n```\n\n\n\n","n":0.06}}},{"i":17,"$":{"0":{"v":"Introduction","n":1},"1":{"v":"[[MSE]] Topic: [[DataMgmt]]\n\n# Poliglot Persistence\nYou should use different Databases to takle different Problems. \n\n![Poli](assets/images/Poliglot_Persistence.png)\n\n- Polyglot persistence is about using different data storage technologies to handle varying data storage needs.\n- Polyglot persistence can apply across an enterprise or within a single application.\n- Encapsulating data access into services reduces the impact of data storage choices on other parts of a system.\n- Adding more data storage technologies increases complexity in programming and operations, so the advantages of a good data storage fit need to be weighed against this complexity.\n\n$$\\nearrow$$","n":0.108}}},{"i":18,"$":{"0":{"v":"CTE","n":1},"1":{"v":"# Common Table Expressions\n\nCTE can be used with recursive clause to deal with hierarchical or tree structured data.\n``` sql\nWITH movies_etp AS (\nSELECT * from movies WHERE name ilike ‘P%’\n),\n--tmp2 AS (\n--…\n--)\nSELECT * from movies_etp ORDER BY name;\n\n```\n\nfor recursive calls:\n```sql\nWITH RECURSIVE query_name [ (column_name [,...]) ] AS (\n-- non-recursive term\nSELECT …\nUNION ALL\n-- recursive term\nSELECT …\n)\nSELECT ...;\n```\n\nA datatype defines possible values - e.g. Int = integer numbers but defines as well how data is stored and indexed.\nContains as well constructors, operators, functions and optimizers (with index extensions)\n\n","n":0.108}}},{"i":19,"$":{"0":{"v":"Arraytypes","n":1},"1":{"v":"\n# Array types\n- Arrays are a sort of collections\n- They have a fixed or variable size\n  \nIn pg - any datatype can be stored as array and are **index 1 based**!\n\n```sql\nCREATE TABLE sal_emp ( \n    name text, \n    pay_by_quarter integer[], \n    schedule text[][]  -- two dimensional array\n);\n\n\nINSERT INTO sal_emp VALUES (\n'Bill', \nARRAY[10000, 10000, 10000, 10000], \nARRAY[['meeting', 'lunch'], \n['training', 'presentation']]\n); \n\n-- Works as well\nSELECT ARRAY[1,2,3+4]; \n-- returns an array with values {1,2,7}\n\n\n-- equal: =\nSELECT ARRAY[1,3] = ARRAY[1,3];\nSELECT ARRAY[1.1,2.1,3.1]::int[] = ARRAY[1,2,3];\nSELECT ARRAY[3,1] = ARRAY[1,3]; -- !!\n-- is contained by: <@\nSELECT ARRAY[3,1] <@ ARRAY[1,3]; -- !! \nSELECT ARRAY[2,7] <@ ARRAY[1,7,4,2,6];\n-- overlap - have at least one element in common: &&\nSELECT ARRAY[1,4,3] && ARRAY[2,1];\n-- ANY syntax\nSELECT * FROM sal_emp\nWHERE 30000 = ANY (pay_by_quarter); \n\n\n--- is the value 3 and 77 and 1 in the array 3,1,4\nSELECT ARRAY[3,77,1] <@ ARRAY[3,1,4];\n-- will output false!\n\n```\n\n\n","n":0.085}}},{"i":20,"$":{"0":{"v":"Antede","n":1}}},{"i":21,"$":{"0":{"v":"Textclassification","n":1},"1":{"v":"# Text Classification\nSentitment analysis is not simple text classification\n\n- Lot of people still do hand coded rules to classify text: Needs a lot of work (expert knowledge)\n- Supervised machine learning (Labeling is required)\n  - Learn the relationship between text and label\n\n- a set of document $D$\n- a fixed set of $N$ classes $C$\n- a training set of $M$ hand-labeled documents ($d_1$, $c_1$), . . . ,($d_M, c_M$)\n\n- a mapping $D \\to C$ that associates a predicted class $c \\in C$ to each\ndocument $d \\in D$\n\n- Naïve Bayes\n- Logistic regression\n- Support-vector machines (SVM)\n- k-Nearest Neighbors\n\n![Naïve Bayes concept](/assets/images/2022-03-03-09-28-57.png)\nIts called naive because it assumes that the features are independent of each other. \n![Bayes theorem](/assets/images/2022-03-03-09-31-50.png)\n\n[[MAPClassifier]] is also called maximum a posteriori (MAP) classifier.\n\n$argmax_{c \\in C} \\frac{P(d|c)P(c)}{P(d)} = argmax_{c \\in C} P(d|c)P(c)$\n\nBernoulli model (if word is in document) or multinominal model when $f_i$ is a count of the number of times the word $i$ appears in the document.\n\nNaive bayes is the probability of class c with the product of conditioanl probabilities of each word in the document.\n\n$$c_{nb} = \\text{arg max}_{c \\in C} P(c) \\prod_{k = 1}^N P(x_k | c)$$\n\n**Slide 18** every class observed $\\frac{1}{3}$\n\n## Evaluation\n\n### Accuracy\n### Precision\nFraction of test documents classified as c that have been correctly classified - only look at the predicted class\n\n$$P_c = \\frac{TP}{TP + FP}$$\n### Recall\nThe fraction of test document labeld as c that have been correctly classified - look a the test labels given by the data\n\n$$R_c = \\frac{TP}{TP + FN}$$\n### $F_1$ Score\nHarmoic mean of precision and recall\n\n### Confusion Matrix\nGives an overview about the performance of your model. \n\n![Confusion Matrix](/assets/images/2022-03-10-09-50-12.png)\nAccuarcy is 2/3 - sum of diagonal from top right to bottom left divided by the overall sum.\n\nPrecision: given a column (i.e., a predicted label), element on main diagonal over sum of matrix column - geneva example: $\\frac{1}{1}$\n\n\nRecall: given a row (i.e., a test label), element on main diagonal over sum of matrix row - bern example $\\frac{1}{2}$\n**Only works this when test labels on the left handed side and predicted labels on the bottom - caution - it could flip!**\n\n# Final Exam Help\n- [ ] Recalculate all the Metrics\n\n[[mse.ss22.finalexams]]\n\n\n\n","n":0.054}}},{"i":22,"$":{"0":{"v":"Sentimentanalysis","n":1},"1":{"v":"# Sentiment Analysis\n\n\nTarget is to understand the emotional state of the author of a text, which can be **extremly hard.**\n\nCan be misleading : \n![Missleading example using sentiment analysis](/assets/images/2022-03-10-10-47-08.png)\n\n- The following thing have the be taken into accoutn:\n- opinion holder: whose opinion is this?\n- target entity: what is this opinion about?\n- aspect: specific feature of the target that this opinion is about\n- type: most commonly positive, negative, neutral, along with an indication of strength\n- time when this opinion was expressed\n\n![Example of entities](/assets/images/2022-03-10-10-49-51.png)\n\n## Dog Example\n![Dog Example](/assets/images/2022-03-10-10-51-35.png)\n\n- There are problems with negations\n> I didn't like it.\n- There are problems with modal words\n> Many consider the movie bewildering, boring or slow-moving or annoying\n- Not literal language \n> Not exactly a masterpiece\n\n## Simplify the problem \n\n- Subtlety: Let’s all go to see Miss Hepburn and hear her run the gamut of emotions from A to B! Dorothy Parker (1893-1967), American writer (criticizing the American actress Katharine Hepburn in 1934)\n\n- Sarcasm: The staff went out of their way to make us look forward to\nthe end of our vacation.\n\n## Approach\n1. tokenization (often hard with informal text)\n1. feature extraction\n1. classification (NB, MaxEnt, SVM)\n1. two or three classes: {positive | negative | (neutral)}\n\nIn Sentitment-Analysis topic words are not as importat as sentitment words such as splendid (prächtig) and bad.\n\nTurney (2002) startet it using a POS-Tagger (1), then estimate the **semantic orientation** (2) and then **classified on the average semantic orientation**\n\n## PMI\nSemantic orientation is computed based on Pointwise Mutual Information (PMI)\n![Probability calculation](/assets/images/2022-03-10-11-25-36.png)\n\n![Calculation of the PMI](/assets/images/2022-03-10-11-28-52.png)\nWhere the second #Academy should be #Award\n\n- Words with high PMI are very likely to occur jointly\n- Two words with low PMI are very likely to occur separately\n- The PMI of select _bigrams_ with reference words is computed\n- excellent for positive reviews and poor for negative reviews is poor\n\n![Phrase - PMI Calculation](/assets/images/2022-03-10-11-32-39.png)","n":0.058}}},{"i":23,"$":{"0":{"v":"Intro","n":1},"1":{"v":"[[MSE]] [[AnTeDe]]\n# Introduction\nBanks want to do automatical summaries of investment reports.\n\nIt is important to take to correct metric for getting the right result. F1_Score is not always the best metric.\n\n## How you should proceed with NLP-problems\n1. Frame the problem and look at the big picture.\n1. Get the data.\n1. Explore the data to gain insights.\n1. Transform the data to better expose patterns to ML algorithms.\n1. Explore different ML models and shortlist the best ones.\n1. Fine-tune your models and combine them into a solution.\n1. Present the solution to colleagues.\n1. Launch and monitor the system.\n\n![How to proceed with NLP](assets/images/AnTeDe1_Proceed.png)\n## Data preparation\n1. Train a model with the data and evaluate it based on the validation / development set.\n1. Get insights how your selected model will perform with the test-data (**ONLY AT THE END WHEN THE MODEL IS SELECTED**)\n\n\n## POS-Tagging\nCheck for the different kind of Tags (There are 9 Categories). You can tag them by **rules**, with **neural networks** etc... Caution: Words are created all day (nouns, verbs, adjectives...).\n\n## Tokenization / Sentence Segmentation\nYou can combine them to fix issues with *Mr. Jones* which could by a sentence split by easy rules.\n\n","n":0.073}}},{"i":24,"$":{"0":{"v":"Document Representation and Querying","n":0.5},"1":{"v":"\nUser has information need and expresses it as query.\n\nIn the boolean model we are very limited.\n\ncreate representations  with binary term document incident matrix\n## Boolean model\n- documents can not be ranked\n- does not return documents containing variants of the query words\n- users must understand the operators\n\n![Relationships](/assets/images/2022-03-17-09-26-58.png)\n\n## Goal to have ranks / full-text queries\nShould also provide partial matching\n\n![](/assets/images/2022-03-17-09-29-08.png)\n\nFunctional words in the sentiment analysis are words like \"not good\" - these are important in sentiment analysis.\n\nterm-document matrix is a matrix counting the words appearing in a document. -**we have a lots of zeros and want to fix it :smile:**\n\nWe represents document in the vector space.\n\n\n![How to constrcut a word-document matrix](/assets/images/2022-03-17-09-46-41.png)\n\nAdaptions in term frequency - adapt with a function - 50 times occurance does not mean 50 times as important than occuring once\n![math functions tf](/assets/images/2022-03-17-09-53-22.png)\ninverse document frequency - try to smooth it with operations\n![math operations on idf](/assets/images/2022-03-17-09-53-41.png)\n\n!(Calculations TF-IDF)[https://web.stanford.edu/~jurafsky/slp3/]\n\nDocument length normalization - penalities for long documents\n\n\n## Cosine similarty \n$$\\frac{\\sum_{i=1}^N v_i \\times w_i}{\\sqrt{\\sum_{i = 1}^N v^{2}_i}\\sqrt{\\sum_{i = 1}^N w^{2}_i}}$$\n\n\n![Okapi BM 25 formula](/assets/images/2022-03-17-10-27-51.png)\n$$\\text{score}(q,d) = \\sum_{r \\in q}\\frac{tf \\cdot idf (t,d)}{|d|}$$\n**Do an tf-idf for each word in the query**\n\n\n\n\n\n# Machine Learning models for ranked retrieval\n- Fast\n- robustness\n\n## Learning to rank (LTR)\n\n- **pointwise approach (learn scores)**\n  - traditional ML approaches, e.g., One-Class SVM, P-rank\n- **pairwise approaches (also learn scores, but with different loss functions)**\n  - consider pairwise training data, i.e. positive examples: ym > yn for a pair of documents and a given query, use pairwise loss to learn a classification/regression model\n- **listwise approaches (consider the ranked list in its entirety)**\n  - consider the ranked list in its entirety\n\n# Takeaways\n- We need data\n- We need people who annotate data\n- we need people who can understand the data\n\n![Metrics for Ranking](/assets/images/2022-03-17-10-52-09.png)\n \nDiscounted cumulative gain (DCG) - gives high ranks a higher score but also takes low ranks into account\n\n**thereotically-grounded model**\n","n":0.058}}},{"i":25,"$":{"0":{"v":"Asdf","n":1},"1":{"v":"[[MSE]] Topic: \n$\\beta = 1$\n\n","n":0.447}}},{"i":26,"$":{"0":{"v":"Sw03","n":1},"1":{"v":"# Semester week 3\n","n":0.5}}},{"i":27,"$":{"0":{"v":"Sw01","n":1},"1":{"v":"# Semesterweek 1\n","n":0.577}}},{"i":28,"$":{"0":{"v":"Privlaw","n":1}}},{"i":29,"$":{"0":{"v":"Why privacy and law","n":0.5},"1":{"v":"\n# Why privacy and law\nFor companies it is a huge **reputation risk** if they misuse personal data!\n\nBig change incoming with the new privacy law (GDPR - General Data Protection Regulation)\n\n# Data Protection\n- is defined as the protection of personal integrity agains infringements (Civil Law - ZGB Art. 28)\n- **established** the second half of the 20th century\n- means\n  - informational **self-determination**\n  - protection of the **personality** in the data-processing\n  - protection of **privacy**\n  - protection form **abusive data processing**\n\n\n- [ ] insert theory of spheres\n\n\nAny infringement (Verletzung) of the integrity/personality is illegal unless it is justified by the consent of the person whose rights are infringed or by law or by an overriding private or public interest\nThen you could go to the court\n\n- Swiss Federal Act on Data Protection (DSG) will come into force in 2022.\n- Canton and communal: Every canton has its own flavor.\n\nNew Data Protection law only applies to natural persons by \n- natural persons (**natural and juristic personen!!!**)\n- federal bodies\n\nIt does not apply to\n- personal data that is processed by a naturla person _exclusivly for personal use_ and which is not _disclosed to outsiders_\n- deliberations of the federal assembly and in parliamentary committees\n\nRecording is different - see privacy notice in teams\n\n# Territorial scope\nSwiss law is now applicable in foreign coutnries! **Marktortprinzip**\n> **This act is applicable to fact patterns that have an effect in switzerland, _even if they occured abroad_ !!!**\n\n# **revDSG**\n## **Article 5** - definitions\n- personal data\n- data subject\n- sensitive personal data\n- processing\n- **Controller** private person (human or company) that decides on the purpose and the means of the processing - **so not the processor is the controller but their boss/company which gave the order to process the data**\n- **processor**\n\nData must be destroyed when it is no longer needed regarded to the purpose (e.g. apply for a new job)\n\nAnyone who processes personal data must ascertain that the data is accurate. \n\nExample:\nMiss Florian Bär $\\to$ i have the right that this data is corrected!!\n\nYou have to inform the people how you use their data!\n\n## Proportionate\n\nDo the reason of the collected personal data justify the penetraion in ones privacy?\n\nImportant regarding websites - Cookie Buttons must look the same - you can not guide the use one of the buttons.\n\n## data security\nthe controller and the processor must ensure (sich versichern) with adequate technical and organisational measures, security concerns are addressed!\n\n## inventory of processing activities\ncontroller and processor keep an inventroy of their processing activities\n\n## notification\nThe controller shall notify the FDPIC (Datenschutzbeauftragter) as soon as possible after a security breach\n\n# access rights (Art. 25 revDSG)\n- send the data back and destroy all the data\n  \nAnybody has the right when personal data of him is collected and for what purpose it is used\n- for how long\n- how as access\n- purpose\n\nactual data protection law goes further than the new DSG\n\n\n\n## limitations\nthe controller has the right refuse, restrict or defer provision of information\n- is manifestly nfounded in particular if it pursues a purpose that is contrary to data protection or is obviously of a frivolous nature!\n\nThe data subject shall receive the information required in order to enable him to assert his rights under this Act and to ensure the transparent processing of data.\n- **in 30 day**\n- **free of charge**\n\n\nAny person may request from the controller, free of charge, the disclosure of the personal data that he has isclosed to him in a standard electronic format if:\n- the controller processes the data in an automated manner; and\n\n# Sactions / Penalties\nFines up to 250'000 Swiss Francs are quite small compared to other european countries!\n\n328b OR (code of obligation)\n- The employer may data about the employee only process as it concerns his qualification for the employment or are inevitable for the execution of the employment. The regulations of the Swiss Data Protection Act are applicable.\n\n# Simple case\n> Nova-AG, headquartered in Dornbirn (AT), has developed a software that enables large companies to automatically analyse the absences of their employees. HR managers automatically receive profiles of their employees and their absences through the system and can thus intervene in a supportive manner. Nova AG has a subsidiary in Switzerland, which sells the software and the service hosted in Austria.\n>\n What is the data protection situation?\n\n\nSwiss Emplyoees are protected by the revDSG and austrian employees are protected by the GPDR.\n\n","n":0.038}}},{"i":30,"$":{"0":{"v":"Tutorial","n":1},"1":{"v":"\n# Tutorials are listed below","n":0.447}}},{"i":31,"$":{"0":{"v":"Sw01","n":1},"1":{"v":"[[mse.ss22.sw01]]\n\n# Case 1\n\n> As engineer you’re responsible for the evaluation of a new heating system for your industrial plant. Any legal requirements?\n\nYes indeed. First of all I must do my job with \"due diligence\" while doing the evalution. Then I must follow the standards that nobody gets hurt while operating these systems when they will be installed. Further I can not prefer a company where I have a friend of mine working at without objective reason.\n\n# Case 2\nYou developed an injection moulding machine that produces „tailor-made“ plastic eye lenses. The process needs a sub-millimetre handling. To achieve that you use an optical camera. The producer of the camera developed for that specific machine an optical recognition software.\n> Which main legal problems can you recognise? Discussion.\n\nWhat happens if the camera breaks or is wrongly calibrated and a user of these plastic lenses gets hurt? \nFurther you have to ensure you get all the medical certifications needed for the country you want to sell these lenses. What if you don't do the certifications?\n\n# Case 3\nGoogle Maps shows the actual traffic situation. You plan to use this information to categorise areas that are noisier and therefore real estates have a low worth. You integrate this information into your platform and sell it online.\n> Any legal questions?\n\nThe data obtained by google is proably owned by google, so you can not sell data collected by google services without their grant. I think Google could bring you to justice for this use of their data.\n\n# Case 4\nIn your development and testing of a medical device you collect a lot of sensitive medical data. Accidentally you find out that your colleague uses this data to train his AI-algorithm.\n\n> What do you think? \n\nI guess you can't just take data out of a testing device for you own machine learning models. First you have to ask the producer of the medical devices if you could provide you with the data for this device. Further the patients which provide (probably without knowing) the data have to accept that their data is used for training an online machine learning model. \n","n":0.053}}},{"i":32,"$":{"0":{"v":"Introduction PrivLaw","n":0.707},"1":{"v":"\n# Introduction [[PrivLaw]] [[MSE]]\ns\n## Why law?\nLaw are not only books - there are agreements for rules as well.\n\nLaw is social framework \nRules save energy - compared to Parents and children\nWithout **Risk** - most businesses are not successful.\nLawyer should not only support one side - more find the best solution for both sides of a given problem.\nIn technical projects legal support is necessary as early as possible! Otherwise they might be **shot off** in the very last minute!\nBy law the management is personally responsible to organise & control the legal compliance! (Art. 754 OR)\n\n# Privacy and data protection\nPrivacy and the right to forget is essential to develop personally (without social control)\nIt protects people\nWe spread data everywhere with the risk for loosing reputation.\nPersonal data has a financial worth.\nYoung generations are more sensitive to privacy than the boomers.\n\n![](assets/images/SW01_Contracts.png)\nYou have to check in an early stage which law fields are touched and where you can adapt (if it is not already strictly regulated and stirctly defined)\n\nA statement must be justified by an article and proof it with evidence $X = f_{\\text{(Art. XXX) + (evidence YYY)}}$\n**It is not enought to say which article is important, say as well why and because of which evidence you state you argument**\n\n# Spearation of powers\n1. Legislative (people, parliament) $\\to$ Club members\n2. Executive (BR, RR, authorities) $\\to$ Vorstand\n3. Judiciary (courts) $\\to$ Controllers (Kontrollstelle)\n\nClubs are organized the same way (see above)\n\n**In reality the ececutive has large discretion** as long as nobody appeal against order (large possibility to adapt and decide)\n\n# Switzerland\n- The people and the cantons building the swiss confederation and not vice-versa\n- Cantons are in their power of legislation $\\text{\\underline{superior}}$ to the swiss confederation - its not top down, its bottem up (cantons to Country)\n- **public authority is only entitled to legislate and act in a territory if it is $\\underline{\\text{\\textbf{explicitly constitutional legitimation}}}$**\n\n![hierarchy of law](assets/images/SW01_hierarchy_of_law.png)\nDecree & Law means $\\to$ Verordnung and Gesetzt\n\n![civil and public law](assets/images/SW01_civil_and_public_law.png)\n\nCivil law is for Contracts\npublic law is for StGB,FMG,BÜPF/VÜPF,ElDI-V u.a.m.\n\n# private / public Law\nCivil law is mastered by **principle of freedom** of coalition & freedom\n\nYou have to recognize if you touch civil law or public law (ignore crime law at the moment)\n\nBy-LAW (Verordnung) $\\ne$ order (Verfügung)\n**Burden of proof** $\\to$ you have to proof it !!\n**if it becomes complicated - start to produce paper (mails, pictures, ask for names etc.) IT MUST BE USED AS PROOVE**\n\nThe **IPRG (Gesezt über das internationale Privatrecht)** is the gateway between swiss & foreign law.\n\nCivil law - the court will ask about an advance on court costs $\\to$ if you ask for millions, the cost in advance will be higher.\n\nHowever looses a civil case - you must pay:\n- court cost\n- other party costs (lawyer)\n- and your own costs\n\nif you win the process - you don't have the money yet. Opposite party can go into bankruptcy\n\n","n":0.046}}},{"i":33,"$":{"0":{"v":"GDPR","n":1},"1":{"v":"[[mse.ss22.sw03]]\n# GDPR (GENERAL DATA PROTECTION REGULATION)\nSwiss companies is applicable on swiss companies if they offer products to EU / EWR and handle data of their customers or if they exploit the personal data of website visitors\n\nGDPR only represents standardised european data protection law for existing principles! It is a minimal standard.\n\n![GDPR from swiss side](/assets/images/2022-03-09_17-32-50.png)\n\n**Fines up to € 10-20 Millions**\n\nThe measures have to be proportionate and effective.\n\nIt is appliable to the big territorial scope\nRight of access\n\n## WIX Customers\nCaution what you write in to the data protection!\nCaution when sending newsletters!\n\n[DatenschutzSelfAssessmentTool](DSAT.ch)\n\n## GDPR in Details\n\n- Augmenting peoples rights Art. 5/6 GDPR\n- Data storing only as long as necessary Art. 5 GDPR\n- Data protection by design and by default Art. 25 GDPR\n- Big Data: protectoin impact assessment Art. 35 GDPR\n- duty of notification of a personal data breach Art. 34 GDPR\n- designation of a data protection officer Art. 36 GDPR\n- processing is to be carried out on behalf of the controller: only with sufficient guarantees (28)\n- right to data protability in a structured commonly used and machine-readable format (20)\n- responsibility of the controller and duty to implement and document approriate technical and organisational measures (24)\n- No sub-sub-processing without written consent of the responsible (Art. 28 Abs. 2 GDPR)\n\n## What to do\n![What to do](assets/images/2022-03-09_17-57-35.png)\n\n## Audit\n- Which personal data do exist?\n- In which form and where? For what reason?\n- Who is responsible? Who has access?\n- How long will be the data stored?\n- sHow are they protected technically and organisationally?\n- What are estimated the possible risks of a data breach and their consequences?\n\n**The GDPR requires to adapt existing contracts (i.e. consent), declarations (terms of use) and proceedings.companies have an augmented duty to document!**\n\n\n![Risk Map](assets/images/2022-03-09_18-25-03.png)\n![ToDo List](assets/images/2022-03-09_18-27-55.png)\n\n**TOM = Technical & Operational Measures**\n\n## Cookies\nCheck cookies with [Cookiebot](cookiebot.com)\n\n## Innovation\n**Is the gdpr a drawback for innovation?**\n- Yes & No! Innovation is still possible but it requires more legal and technical attention!\n- Chances for new privacy- and data protection-compliant technologies and services!\n- SMEs (small and medium-sized companies) struggle with legal, organisational and technical demands. Big’s (FB, Google etc.) have an advantage.\n- One of the winners are therefore cloud-services (i.e. AWS, Google, MS, IBM), because the can easier manage the systems than a small company!\n\n","n":0.053}}},{"i":34,"$":{"0":{"v":"Predmod","n":1},"1":{"v":"\n# Compliance and Records Management\n\n## Compliance\n### definition\nIts not like coroprate governance\n\nSwiss code $\\to$ best practices for corporate governance (2002 established by economiesuisse)\n\nIt aims to encompass all the principles aimed at safeguarding sustainable company interest. Guarantees transparency and a healthy balance of management and control.\n\nIt has an important role wheter the ceo/board of directors acts professional or not\n\n### 5 key functions\n\n1. identy risks that an organisation faces (identification)\n2. desing and implement controls to protect an organisation from those risks (prevent)\n3. monitor and report the effectiveness of those controls in managment / exposure to risks (monitoring and detection)\n4. resolve compliance difficulties as they occur (resolution)\n5. advise the business on rules and controls (advisory)\n\n#### Definition of risk\n**Risk = proabiity of incident * potential of damage**\n\nDanger not equal to risk\nRisk management is important. \nYou could prevent fatal financial loss - but normally you lose reputation as well.\n\n\n### essentials of risk management\nIdentify risks \n\nGute-Labor-Praxis --> SOP (Standard Operation Procedure)\n\nVUCA\n![VUCA](assets/images/2022-03-16_17-45-21.png)\n\n360° view of risk\n\n![360 risk map](assets/images/2022-03-16_17-47-25.png)\n\n![risk board](assets/images/2022-03-16_17-48-22.png)\n\n\nNot only the main risks are problems. Take effort to find the small risks - especially when your leading a company - you can't see the problems of the \"small men\"\n\nDangerous is not knowing the risks!\n\nConfirmation bias\n\ncockpit syndrome - put pressure on the leading personal!\n\nControl the little risks - they could lead to big problems!\n\ndon't decide on mood - make decision matrics!!\n\nRisk managment map is important to quantify the risks and takle them afterwards\n\n\n\n![Get the middle](assets/images/2022-03-16_18-39-25.png)\n\n### life cycle of records\nRECORDS MANAGEMENT (ISO 15489)\n\n- Create\n- Store\n- Use\n- Share\n- Archive\n- Destroy\n- Create\n- ...\n\nCategorize all documents but be cautios with \"non-essential\"\n- nonessential \n- useful\n- important\n- vital\n\n**Different safekeeping periods - 5 to 10 years**\n\n\n\n\n### essential laws for records management\n\n- 957 ff OR\n- GeBüV\n- MWST (VAT)\n- ZPO\n- ATSG\n- DSG\n- and others\n\nYou need to keep the information **Art. 166 StGB**\n\nBoard of Directors as well **Art 754 OR - code of obligations**\n\nAccounting vouchers with the annualand the audit report for 10 years!!! **Art. 958f OR** \n\n**Art. 2 GeBüV** - Geschäftsbücher Verordnung\n\n\nINTEGRITY (AUTHENTICITY & UNALTERABILITY - **ART. 3 GeBüV**\n\nThe accounts must be kept and retained in such a way and \nthe supporting documents must be recorded and retained in \nsuch a way that they cannot be altered without it being \npossible to establish that they have been altered.\n\n- **ART. 4 GeBüV** Responsibility for infrastructure etc...\n- **ART. 5 GeBüV** - general duties of care\n- **ART. 6 GeBüV** availabitlty\n- **ART. 7 GeBüV** separate the data\n- **ART. 8 GeBüV** organize the data\n- **ART. 9 GeBüV** media and migration (how we need to safe the data)\n  - How the archived data is migration into future data formats!\n\n\n**Art. 9 Abs. 1 VDSG (actual law) or ISO-Certification / GDPR**\n- Protection against external physical influences (mostly natural hazards)\n- Protection against unjustified access, control mechanisms (\"four-eyes principle“)\n\n- How get information about risk issues in a production company\n  - Risk strategy should be available \n  - stay in touch with the production (weekly meetings etc...)\n\n\n","n":0.046}}},{"i":35,"$":{"0":{"v":"Simplelinearregression","n":1},"1":{"v":"# Simple linear regression\nwe get the 95% [[confidence interval]] with the following formula: [$\\hat{\\beta_1} - 2 * \\text{se}(\\hat{\\beta_1}), \\hat{\\beta_1} + 2 * \\text{se}(\\hat{\\beta_1})$]\n\n$\\epsilon = Y - (\\beta_0 + \\beta_1 * X)$ cannot be measured since $\\beta_0$ and $\\beta_1$ are unknown.\n\nApproximation for residuals for $\\epsilon$ **residuals** $r_i = y_i - (\\beta_0 + \\beta_1 * x_i)$\n\n[[RSE]] Residual Standard Error = $\\text{RSE} = \\sqrt{\\frac{r_1^2 + ... + r_n^2}{n - 2}}$\n\nPrediction Interval = $\\text{se}(y_0)^2 = \\sigma^2 (1 + \\frac{1}{n}+ \\frac{(x_0 - \\bar{x}^2)}{\\Epsilon_{i = 1}^2 (x_i - \\bar{x})^2})$\n\n# Model Assumptions for the Error Term\n$\\epsilon_i\\text{ iid }\\N(0,\\sigma^2)$\n\n## Aim of residual analysis\nIf the model contains errors - we get a starting point for further investigation to a better and more adapted model (explorative data analysis)\n\n**RSE** considered a measure of the **lack of fit** of a regression model to the data. \n**$R^2$** Statistic is defined as : $R^2 = 1 - \\frac{\\Epsilon_{i = 1}^n (y_i - \\hat{y_i})^2}{\\Epsilon_{i = 1}^n (y_i - \\bar{y_i})^2} = 1 - \\frac{\\text{variance left after regression fit}}{\\text{total variance}}$\n\n$R^2$ is always between 0 and 1 (0 is bad, 1 is a perfect model)\n\n# [[Correlation Coefficient]]\n$r = Cor(X,Y) = \\frac{\\Epsilon_{i = 1}^n (x_i - \\bar{x_i})(y_i - \\bar{y_i})}{\\sqrt(\\Epsilon_{i = 1}^n (x_i - \\hat{x_i})^2)\\Epsilon_{i = 1}^n (y_i - \\bar{y_i})^2)}$\n\nYou can not use the Correlation because of Multiple Linear Regression\n\n# Diagnostics Tool for Testing Model Assumption - Tukey-Anscombe-Plot\nVerify that $E[\\epsilon] = 0$\n## Tukey-Anscombe-Plot\nDo it by the use of the **Tukey-Anscombe-Plot**\n\n- Residuals ploted on the vertical axis\n- predicted values on the horizontal axis\n\nValues should be in a horizontal line at Residuals = 0\n\nSimulate Points to get more points \n\n![](assets/images/tukey_ascom.png)\n\n## Scale-Location Plot\n\nUse the Standardized residuals \n\n### Consequences for case of correlated error terms\n- The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of independent error terms $\\epsilon_i$\n- If there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be\n\n# Outlier\nAn outlier is if a data point is very far away from the most of the data.\nIf we remove an outlier - we have a small effect on the $\\beta_0$ and $\\beta_1$ but the effects on the **RSE** and $R^2$ is large\n\n## High Leverage points\n![](assets/images/high_leverage_points.png)\nIf we remove observation 41 - the red line is the correct point - so if we remove high leverage points, the impact on $\\beta_0$ and $\\beta_1$ is big.\n\n## Cooks distance\n$\\hat{y}_{-i}$ obeservation if you remove the i-th value.\nAs higher the value of cooks distance is - the higher is the influence of the i-th data point on the dataset\nIf a cooks value higher than 1 is observed - the value is considered as dangerously influencual.\n\n**Read more about Tukey first aid principles**\n\n\n**If you discover outliers and high leverage points - do not just remove them - it could be an important discovery for a new and other model!!!**\n\n1. Check wether outlier has occured due to an error in data collecito or recording\n   1. If an error may have occured: omit the data point\n   2. if an error can be excluded: go to 2\n2. Attempt to transform predictor or response variables in order to make _disappear_ the outlier. If no improvement go to 3.\n3. Outlier occured due to an unusual random varianions: If such outlier affect parameter estimation too much, then the observation my be removed (**needs to be mentioned in the reports**)\n\n","n":0.042}}},{"i":36,"$":{"0":{"v":"Qualitativevariables","n":1},"1":{"v":"# Qualitative Variables\nFirst we need to create a dummy variable\n\n1 if the person is female and 0 if the person is male\n\nSee page 28 in slides\n\n**There is always one fewer dummy variables then the number of levels** as there is a baseline dummy variable\nThere are many ways of doing it - no difference on fit but has effects on the interpretation of coeﬃcients and p-values\n\n","n":0.124}}},{"i":37,"$":{"0":{"v":"Pre Semester","n":0.707},"1":{"v":"[[PredMod]] [[MSE]]\n## Pre-Semster\n### Cumulative Distribution Function\nCumulative Distribution Function is defined as follows: $F(x) = P(X \\le x)$ where $X$ is the set of all possible values of the random variable and $x$ is the value of the random variable at which the CDF is to be evaluated. \nThis is needed to get a range of values e.g. $F(X \\in (a,b]) = P(a < X \\le b)$ which is defined as $F(b) - F(a)$. Important as well. The derivative $F'(x)$ **is always 0 or bigger**\n![Example CDF](assets/images/Example_CDF.png)\n\n### probability density Function\nThe probability density Function is defined as $f(x) = F'(x)$ which is the derivative of the CDF.\nThe PDF must always be **zero or positive** - as the CDF is always monotonically increasing.\n$P(a < X \\le b) = F(b) - F(a) = \\int_a^b f(x)dx$\n\n### expected value\nThe expected value $E(X)$ and the standard deviation $\\sigma_X$ have the same meaining in the discrete and the continuous case.\n$E(X) = \\int_{-\\infty}^{\\infty} x f(x) dx$ whereas $x$ is the outcome and $f(x) dx$ is the probability of the outcome.\n### variance \nThe variance $V(X)$ is defined as $V(X) = E(X^2) - E(X)^2$\n### quantiles\nThe quantiles are defined as $q(a) = 182.5$ which means that 75% of the values are smaller than $q(a)$ and 25% are larger than $q(a)$.\n\n# important continous distributions\n\n## Uniform Distribution\nThe uniform distribution is defined as follows:\n$$\nf(x) = \\left\\{\n    \\begin{array}{ll}\n        \\frac{1}{b-a} & if a \\le x \\le b \\\\\n        0 & else\n    \\end{array}\n\\right.\n$$\n`dunif(x = 5, min = 1, max = 10)` for the density of the uniform distribution at $x = 5$ from $1$ to $10$.\nTo get the probability of $P(1 \\le X \\le 5)$ use `punif(q = 5, min = 1, max = 10)`.\nTo generate random variables use the following code snippet: `runif(n = 5, min = 1, max = 10)`\n\n## Exponential Distribution\nThe exponential distribution is defined as follows:\n$exp(x) = e^x$\n$$\nf(x) = \\left\\{\n    \\begin{array}{ll}\n        \\lambda * exp(-\\lambda x) & if x \\geq 0 \\\\\n        0 & otherwise\n    \\end{array}\n\\right.\n$$\nwhere we write $X \\sim Exp(\\lambda)$\n![Samples of the exponential distribution](assets/images/ExponentialDistribution.png)\nAssuming $X \\sim Exp(3)$ for $P(0 \\le X \\le 4)$ with R it is calculated as follows `pexp(4, rate = 3)`\n\n### Poisson and Exponential Distribution\nThese two distributions are related. If the time elapsed between two failures of a system follows Exp($\\lambda$), then the number of failures during the period t follows Poisson($\\lambda * t$).\n$P(T > t) = P(T \\le t) = P(\\text{no decay in} [0,t]) = \\frac{(\\lambda t)^0 e^{-\\lambda t}}{0!} = e^{-\\lambda t}$\n\n## Normal distribution\nThis is the most important distribution: \n$f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} exp(-\\frac{(x - y)^2}{2 \\sigma^2 })$\nthe cumulative distirbution function cannot be explicitly expressed by formulas and must be calculated with software.\nIQ Example:\n$X \\sim \\mathcal{N}(100, 15^2)$ where 15 is the standard deviation.\nUse R to get the value `1 - pnorm(130, mean = 100, sd = 15) = 0.022` - therefore 2% is highly gifted.\n\n\n\n[[CDF]] [[PDF]] [[Expected Value]] [[Variance]] [[Quantiles]]\n","n":0.046}}},{"i":38,"$":{"0":{"v":"Multiplelinearregression","n":1},"1":{"v":"# Multiple Linear Regression\n[[mse.ss22.SW03]]\n\nExample with advertising. Sales ~ TV looks good with a $log$ transformation of the response variable.\n\nt-statistics = coefficient / std.error\n\n## How to make a prediction of sales given the levels of the three advertising media budgets?\nAdd further coefficiant\n\n$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon$$\n## Example\n$$\\text{sales} = \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\text{radio} + \\beta_n \\text{newspaper} + \\epsilon \\to \\text{graphical representation impossible}$$\n\n\n$$\\text{RSS} = \\sum_{i = 1}^{n} r^2_i \\to r = (y_i - \\hat{\\beta_1} x_{i 1} - \\hat{\\beta_2} x_{i 2} ...)^2$$ \n\n\n**No statistical evidence Page 9 - Newspapers coefficiant is 0 - p value quite hight - null hypothsis can not be rejected -- Newspapers correlates with Radio or TV data**\n\n![correlation newspapers and radio](/assets/images/2022-03-09-09-34-01.png)\n\nDo all predictors $X_1$ to $X_n$ help to get the response variable or only a subset of them? How well does the model fit the data?\n\n$$E(\\frac{\\text{RSS}}{n - p - 1}) = \\sigma^2 = E(\\frac{\\text{TSS} - \\text{RSS}}{p}) \\to \\text{if the null hypothesis is true}$$\n**else it would be**\n$$E(\\frac{\\text{TSS} - \\text{RSS}}{p}) > \\sigma^2$$ \n**! Where p is the number of predictor variables !**\n\n**F statistics with a value of 570 means we can reject the null hypothesis in [[mse.ss22.predmod.mlr]]**\n\n## How well does it fit?\n$R^2 = Corr(Y, \\hat{Y})^2$\n\n**The more variables you include into the model, the higher is the expected R^2 value**\n\nDont always look to the R^2 value - look as well to the residual plots\n# [[mse.ss22.predmod.ftest]]\n\n# Qualitative Variables\n[[mse.ss22.predmod.qualitativevariables]]\n\n# Additivity vs Interaction\nAdditivity is not fullfilled in the following example \n![Additivity](/assets/images/2022-03-09-11-00-10.png)\nTo fix this, we can add an interaction term \n![Interaction Term added](/assets/images/2022-03-09-11-01-19.png)\n\n\nThe $r^2$ value with the interaction term is much higher than the one without interraction term\nIf you invest into radio - the next investement into tv will be more effective!\n$(\\hat{\\beta_1} + \\hat{\\beta_2} * \\text{radio}) = 1.9 + 1.1 * radio$\n\nSee example for income with qualitative variable student\nSlope of the regression line changes","n":0.057}}},{"i":39,"$":{"0":{"v":"LMS 2","n":0.707},"1":{"v":"\n# Variance inflation factor (vif)\n\nIf VIF is between 5 and 10 - it will become problematic regarding collinearity\n\nSmallest value of vif is 1\n\n\n# if you want to add a squared term\n```r\nsummary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$r.squared\n```\nAdd the **I** before the squred term!!!\n\n1. plot is tukey ascom\n2. normal plot\n3. scale-location plot\n4. cooks-distance plot\n5. residuals vs leverage\n\n# LMS2.1 shows forward \n\n","n":0.128}}},{"i":40,"$":{"0":{"v":"Multiple Linear Regression","n":0.577},"1":{"v":"# MLR means Multiple Linear Regression\n\n","n":0.408}}},{"i":41,"$":{"0":{"v":"LinearRegression","n":1},"1":{"v":"# LinearRegression\nThere is an Output variable $Y$ and one or multiple Input variable $X_1,X_2,X_3$ which then looks as the following: $Y = f(X_1, X_2, X_3) + \\epsilon$\n![Which model would you choose](assets/images/LinReg1_WhichModel.png)\n\nA cubic model is a linear model as well, as the $\\beta$ terms are linear $\\to$ $Y = \\beta_1 + \\beta_2 * X + \\beta_3 * X^3$\n\n$Y = \\Beta_0 + \\Beta_1 * X $ where $\\beta_0$ is the intercept and $\\beta_1$ is the slope.\n**residual sum of squares RSS** is defined as $\\text{RSS} = $\n**residuals** are defined as $r_i = y_i - \\hat{y}_i$\n\n**standard error** $\\to$ $se(\\hat{\\beta_0})^2 = \\sigma^2 (\\frac{1}{n} + \\frac{\\lineontop{x}^2}{\\sum_{i=1}^n (x_i - \\lineontop{x})^2})$\n\n$\\hat{\\sigma} = \\text{RSE} = \\sqrt{\\frac{RSS}{n-2}}$\n\n# T-Test\nHow many times the standard deviation away from zero must $\\hat{\\beta_1}$ from 0?\nIt follows a T-Distribution with $n-2$ degrees of freedom.\n","n":0.088}}},{"i":42,"$":{"0":{"v":"Introduction","n":1},"1":{"v":"# Introduction\n\nExample from Mr. Hubble - Calculate the age of the universe (SW01 - SW04)\nAnother example is the Classification (SW05-SW10)\nOr Analysis of Time Series (SW11-SW14)\n\n","n":0.2}}},{"i":43,"$":{"0":{"v":"F Test","n":0.707},"1":{"v":"# Test\n$q$ is the number of skipped predictor variables\n![F test statistics](/assets/images/2022-03-09-10-24-13.png)\n\n\n## F Test with q skipped\nF-test with last q items skipped\n![F Test with q](/assets/images/2022-03-09-10-27-09.png)\nNull-Hypothesis will check if the last q predictor variables are 0\n\n![F Test for a subset](/assets/images/2022-03-09-10-27-24.png)\n\n**Null Hypothesis is is the coefficient of the kicked preditor is zero: Question means is there evidence that the kick predictor has influence to the response variable?**\n\n\n\n","n":0.125}}},{"i":44,"$":{"0":{"v":"Help for final Exams","n":0.5},"1":{"v":"# AnTeDe\n- [ ] Recalculate all the metrics (Accuracy, Precision, Recall, $F_1$-Score)\n\n\n","n":0.289}}}]}
